services:
  spark-master:
    image: apache/spark:4.1.0-preview4-scala2.13-java21-python3-r-ubuntu
    container_name: spark-master
    # Start the Spark Master process explicitly
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
      --host spark-master
    ports:
      - "8080:8080" # Expose Spark Master Web UI
      - "7077:7077" # Expose Spark Master RPC port (workers & drivers connect here)
    networks:
      - spark-net
    volumes:
      # Volume format: <host_path>:<container_path>
      # This mounts the ./data (./scripts) directory from the host
      # into the /opt/spark/work-dir/data (/opt/spark/work-dir/scripts) directory inside the container
      # We use /opt/spark/work-dir as the mount point to ensure proper permissions
      # for Spark to create temporary files during Parquet writing operations
      - ./data:/opt/spark/work-dir/data
      - ./scripts:/opt/spark/work-dir/scripts

  spark-worker:
    image: apache/spark:4.1.0-preview4-scala2.13-java21-python3-r-ubuntu
    container_name: spark-worker
    depends_on:
      - spark-master
    # Start the Spark Worker process and point it to the master URL
    command: >
      /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker
      spark://spark-master:7077
    environment:
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
    ports:
      - "8081:8081" # Expose Worker Web UI
    networks:
      - spark-net
    volumes:
      # Same volume mapping for the worker
      # Using /opt/spark/work-dir as mount point to ensure proper permissions
      # for Spark operations in the worker nodes
      - ./data:/opt/spark/work-dir/data
      - ./scripts:/opt/spark/work-dir/scripts

networks:
  spark-net:
    driver: bridge